{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3768aa3",
   "metadata": {},
   "source": [
    "# My first NLP Project \n",
    "В проекте буду использовать библиотек nltk, sklearn, numpy, pandas, matplotlib, seaborn\n",
    "\n",
    "1. Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96accd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "aa97ea48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Собаке - собачья смерть\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>тебя не убедил 6-страничный пдф в том, что Скр...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14407</th>\n",
       "      <td>Вонючий совковый скот прибежал и ноет. А вот и...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14408</th>\n",
       "      <td>А кого любить? Гоблина тупорылого что-ли? Или ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14409</th>\n",
       "      <td>Посмотрел Утомленных солнцем 2. И оказалось, ч...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14410</th>\n",
       "      <td>КРЫМОТРЕД НАРУШАЕТ ПРАВИЛА РАЗДЕЛА Т.К В НЕМ Н...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14411</th>\n",
       "      <td>До сих пор пересматриваю его видео. Орамбо кст...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14412 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  toxic\n",
       "0                   Верблюдов-то за что? Дебилы, бл...\\n    1.0\n",
       "1      Хохлы, это отдушина затюканого россиянина, мол...    1.0\n",
       "2                              Собаке - собачья смерть\\n    1.0\n",
       "3      Страницу обнови, дебил. Это тоже не оскорблени...    1.0\n",
       "4      тебя не убедил 6-страничный пдф в том, что Скр...    1.0\n",
       "...                                                  ...    ...\n",
       "14407  Вонючий совковый скот прибежал и ноет. А вот и...    1.0\n",
       "14408  А кого любить? Гоблина тупорылого что-ли? Или ...    1.0\n",
       "14409  Посмотрел Утомленных солнцем 2. И оказалось, ч...    0.0\n",
       "14410  КРЫМОТРЕД НАРУШАЕТ ПРАВИЛА РАЗДЕЛА Т.К В НЕМ Н...    1.0\n",
       "14411  До сих пор пересматриваю его видео. Орамбо кст...    0.0\n",
       "\n",
       "[14412 rows x 2 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"labeled.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e561ec3",
   "metadata": {},
   "source": [
    "# 1. Стоп слова, Токенизация текста, Стемминг и Лиммитизация\n",
    "Используем nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "37a4f962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "st = nltk.PorterStemmer()\n",
    "lm = nltk.WordNetLemmatizer()\n",
    "\n",
    "stop_words = set(stopwords.words(\"russian\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "02b08fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d3b6998b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "      <th>comment_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>верблюдов дебилы бл</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>хохлы это отдушина затюканого россиянина мол в...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Собаке - собачья смерть\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>собаке собачья смерть</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>страницу обнови дебил это оскорбление доказанн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>тебя не убедил 6-страничный пдф в том, что Скр...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>убедил 6 страничный пдф скрипалей отравила рос...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14407</th>\n",
       "      <td>Вонючий совковый скот прибежал и ноет. А вот и...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>вонючий совковый скот прибежал ноет а сторонни...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14408</th>\n",
       "      <td>А кого любить? Гоблина тупорылого что-ли? Или ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>а кого любить гоблина тупорылого или какую про...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14409</th>\n",
       "      <td>Посмотрел Утомленных солнцем 2. И оказалось, ч...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>посмотрел утомленных солнцем 2 и оказалось это...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14410</th>\n",
       "      <td>КРЫМОТРЕД НАРУШАЕТ ПРАВИЛА РАЗДЕЛА Т.К В НЕМ Н...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>крымотред нарушает правила раздела т к в нем н...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14411</th>\n",
       "      <td>До сих пор пересматриваю его видео. Орамбо кст...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>до сих пор пересматриваю видео орамбо кстати с...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14412 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  toxic  \\\n",
       "0                   Верблюдов-то за что? Дебилы, бл...\\n    1.0   \n",
       "1      Хохлы, это отдушина затюканого россиянина, мол...    1.0   \n",
       "2                              Собаке - собачья смерть\\n    1.0   \n",
       "3      Страницу обнови, дебил. Это тоже не оскорблени...    1.0   \n",
       "4      тебя не убедил 6-страничный пдф в том, что Скр...    1.0   \n",
       "...                                                  ...    ...   \n",
       "14407  Вонючий совковый скот прибежал и ноет. А вот и...    1.0   \n",
       "14408  А кого любить? Гоблина тупорылого что-ли? Или ...    1.0   \n",
       "14409  Посмотрел Утомленных солнцем 2. И оказалось, ч...    0.0   \n",
       "14410  КРЫМОТРЕД НАРУШАЕТ ПРАВИЛА РАЗДЕЛА Т.К В НЕМ Н...    1.0   \n",
       "14411  До сих пор пересматриваю его видео. Орамбо кст...    0.0   \n",
       "\n",
       "                                             comment_new  \n",
       "0                                   верблюдов дебилы бл   \n",
       "1      хохлы это отдушина затюканого россиянина мол в...  \n",
       "2                                 собаке собачья смерть   \n",
       "3      страницу обнови дебил это оскорбление доказанн...  \n",
       "4      убедил 6 страничный пдф скрипалей отравила рос...  \n",
       "...                                                  ...  \n",
       "14407  вонючий совковый скот прибежал ноет а сторонни...  \n",
       "14408  а кого любить гоблина тупорылого или какую про...  \n",
       "14409  посмотрел утомленных солнцем 2 и оказалось это...  \n",
       "14410  крымотред нарушает правила раздела т к в нем н...  \n",
       "14411  до сих пор пересматриваю видео орамбо кстати с...  \n",
       "\n",
       "[14412 rows x 3 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tazartu(text):\n",
    "    n_zhok = text.replace(\"\\n\", \"\")\n",
    "    text_tazar = \"\".join([i for i in n_zhok if i not in string.punctuation])\n",
    "    token = re.split(\"\\W+\", text)\n",
    "    stop_w = [j.lower() for j in token if j not in stop_words]\n",
    "    sting = [st.stem(w) for w in stop_w]\n",
    "    lmize = \" \".join([lm.lemmatize(k) for k in sting])\n",
    "    return lmize\n",
    "\n",
    "df[\"comment_new\"] = df[\"comment\"].apply(lambda x: tazartu(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41c3b31",
   "metadata": {},
   "source": [
    "# Векторизация слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a75d3b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "64f2fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(analyzer=tazartu)\n",
    "X = vect.fit_transform(df[\"comment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ff85a71f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14412, 68440)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9639b063",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = CountVectorizer(ngram_range=(2,2))\n",
    "X_c = ngram.fit_transform(df[\"comment_new\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4fae26cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14412, 224508)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df32d1c8",
   "metadata": {},
   "source": [
    "# TF-IDF и Обучение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b2986ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    n_zhok = text.replace(\"\\n\", \"\")\n",
    "    text_tazar = \"\".join([i for i in n_zhok if i not in string.punctuation])\n",
    "    token = re.split(\"\\W+\", text)\n",
    "    stop_w = [j.lower() for j in token if j not in stop_words]\n",
    "    sting = \" \".join([st.stem(w) for w in stop_w])\n",
    "    return sting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e8c9b3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8327486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(analyzer=clean)\n",
    "X_tf = tf.fit_transform(df[\"comment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "db11b8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14412, 70)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "30a44a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14412x70 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 344553 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a8ab1fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['comment_new']\n",
    "y = df['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "051960cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy  0.8629899410336455\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "\n",
    "svm_mod = make_pipeline(TfidfVectorizer(), SVC(kernel='linear', probability=True))\n",
    "svm_mod.fit(X_train, y_train)\n",
    "pred_svm = svm_mod.predict(X_test)\n",
    "print(\"SVM accuracy \",metrics.accuracy_score(y_test, pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "97adc798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression  0.8168574401664932\n"
     ]
    }
   ],
   "source": [
    "lgr = make_pipeline(TfidfVectorizer(), LogisticRegression())\n",
    "lgr.fit(X_train, y_train)\n",
    "pred_lgr = lgr.predict(X_test)\n",
    "print(\"LogisticRegression \",metrics.accuracy_score(y_test, pred_lgr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c835dc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier  0.751647589316684\n"
     ]
    }
   ],
   "source": [
    "dtc = make_pipeline(TfidfVectorizer(), DecisionTreeClassifier())\n",
    "dtc.fit(X_train, y_train)\n",
    "pred_dtc = dtc.predict(X_test)\n",
    "print(\"DecisionTreeClassifier \",metrics.accuracy_score(y_test, pred_dtc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "09856e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier 0.3260492542490461\n"
     ]
    }
   ],
   "source": [
    "knn = make_pipeline(TfidfVectorizer(), KNeighborsClassifier())\n",
    "knn.fit(X_train, y_train)\n",
    "pred_knn = knn.predict(X_test)\n",
    "print(\"KNeighborsClassifier \",metrics.accuracy_score(y_test, pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "8cf6e1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest  0.8074921956295525\n"
     ]
    }
   ],
   "source": [
    "rfc = make_pipeline(TfidfVectorizer(), RandomForestClassifier())\n",
    "rfc.fit(X_train, y_train)\n",
    "pred_rfc = rfc.predict(X_test)\n",
    "print('RandomForest ', metrics.accuracy_score(y_test, pred_rfc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "7f83db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "with open('model_svm.joblib', 'wb') as file:\n",
    "    joblib.dump(svm_mod, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c69bab47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svcc = joblib.load(\"model_svm.joblib\")\n",
    "test = \"Мирас еблан\"\n",
    "a = tazartu(test)\n",
    "svcc.predict([a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c5ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
